{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset Tracker Tool - Tracking serial numbers among multiple data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tool compares data exports in CVS formate. Examples of datasets that can be compared include:\n",
    "\n",
    " - Installed Base\n",
    " - PageSmart (FMAudit data)\n",
    " - Project Rollout Schedule(s)\n",
    " - Monthly Billing File\n",
    " \n",
    "- Functions:\n",
    " - Phase 1: [current] Compare databases 1-to-1 to identify missing devices.\n",
    " - Phase 2: Data accuracy \n",
    " - Phase 3: Predictive corrections\n",
    " \n",
    "- TO DO:\n",
    " - filter out dynamic, add filter in\n",
    " - convert to cvs\n",
    " - account for removal actions in filter out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Data Files and Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in source_file_names:  4\n",
      "Number of entries in source_file_names_large:  1\n",
      "Number of entries in cust_numbers:  4\n",
      "Number of entries in serial_number_name_variants:  5\n",
      "Number of entries in column_data_types_dates:  2\n",
      "Number of entries in potential_leading_chars:  2\n",
      "Number of entries in non_issue_duplicate_column:  1\n",
      "Number of entries in key_and_value_to_filter_out:  6\n",
      "Number of entries in potential_data_headers_keys:  16\n",
      "I did the thing!\n"
     ]
    }
   ],
   "source": [
    "import unicodecsv\n",
    "\n",
    "#####################################\n",
    "#               Formatting for comparison ease of read\n",
    "#####################################\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "#####################################\n",
    "#       WHICH FILES ARE WE LOOKING AT?\n",
    "#####################################\n",
    "'''\n",
    "    src files are standard exports, typically <1mb\n",
    "    src_large_n files are typically larger than a 1mb, up to 15mb\n",
    "'''\n",
    "source_file_names = [\n",
    "    'MGM IB by 4 customer numbers 8.30.3019.csv',\n",
    "    'MGM FMA 8.15.2019 Device Change Worksheet.csv',\n",
    "    'MGM Project ROS Summary.csv',\n",
    "    'EINVOICE-T0BUBPS-30-SEP-19.csv'\n",
    "    ]\n",
    "source_file_names_large = [\n",
    "    'TABS_DW_RPT_INSTALL_BASE_ALL_NEW_VW_20190924 BRIEF.csv',\n",
    "    ]\n",
    "\n",
    "print(\"Number of entries in source_file_names: \", len(source_file_names))\n",
    "print(\"Number of entries in source_file_names_large: \", len(source_file_names_large))\n",
    "\n",
    "\n",
    "#####################################\n",
    "#       MISC information\n",
    "#####################################\n",
    "#Customer Reference Number for Oracle Installed Base\n",
    "cust_numbers = list()\n",
    "cust_numbers = (\"T0BS49A\", \"T0BUQW1\", \"T0BV7VF\", \"T0BVMSY\")\n",
    "print(\"Number of entries in cust_numbers: \", len(cust_numbers))\n",
    "\n",
    "serial_number_name_variants = list()\n",
    "serial_number_name_variants = ['SERIAL_NUMBER','Serial Number','Toshiba Serial Number','SerialNbr', '\\ufeffSERIAL_NUMBER']\n",
    "print(\"Number of entries in serial_number_name_variants: \", len(serial_number_name_variants))\n",
    "\n",
    "column_data_types_dates = list()\n",
    "column_data_types_dates = ['INSTALL_DATE','Last Report Date']\n",
    "print(\"Number of entries in column_data_types_dates: \", len(column_data_types_dates))\n",
    "\n",
    "potential_leading_chars = list()\n",
    "potential_leading_chars = ['S', 's']\n",
    "print(\"Number of entries in potential_leading_chars: \", len(potential_leading_chars))\n",
    "\n",
    "non_issue_duplicate_column = list()\n",
    "non_issue_duplicate_column = ['Counter Name']\n",
    "print(\"Number of entries in non_issue_duplicate_column: \", len(non_issue_duplicate_column))\n",
    "\n",
    "key_and_value_to_filter_out = list()\n",
    "key_and_value_to_filter_out.append(('CUSTOMER_LOCATION_STATE','MD'))\n",
    "key_and_value_to_filter_out.append(('CUSTOMER_NAME', 'MGM STUDIOS'))\n",
    "key_and_value_to_filter_out.append(('CUSTOMER_NAME', 'MGM GALLERIES LLC'))\n",
    "key_and_value_to_filter_out.append(('CUSTOMER_NAME', 'MGM LIQUOR WAREHOUSE'))\n",
    "key_and_value_to_filter_out.append(('CUSTOMER_NAME', 'MGM MORTGAGE'))\n",
    "key_and_value_to_filter_out.append(('ACTION', 'REMOVE'))\n",
    "\n",
    "print(\"Number of entries in key_and_value_to_filter_out: \", len(key_and_value_to_filter_out))\n",
    "\n",
    "#What are some headings you'd like to see if we find missing assets?\n",
    "potential_data_headers_keys = list()\n",
    "potential_data_headers_keys = ['serial_number', \\\n",
    "                               'ACTION', 'Completed Install Date', 'Last Report Date' , \\\n",
    "                               'Primary', 'STATUS for Delivery / Installation', \\\n",
    "                               'Location Name', 'Completed Install Date' \\\n",
    "                               'Ship To / Company Name','Address1', 'Model', \\\n",
    "                               'Ship-To Name', 'Ship To Address1', 'Install Date', 'Status'\\\n",
    "                               'INSTALL_DATE','CUSTOMER_NAME','CUSTOMER_LOCATION_ADDRESS_1']     \n",
    "print(\"Number of entries in potential_data_headers_keys: \", len(potential_data_headers_keys))\n",
    "\n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing 1: MGM IB by 4 customer numbers 8.30.3019.csv as new list: data_set_1\n",
      "\u001b[4mSample pulled from MGM IB by 4 customer numbers 8.30.3019.csv\u001b[0m, entry from 0th list entry: \n",
      "OrderedDict([('\\ufeffPRODUCT_NUMBER', '63451262'), ('INSTALL_DATE', '4/1/2019'), ('GOLD_FLAG', 'Y'), ('CUSTOMER_REFERENCE', 'T0BS49A'), ('CUSTOMER_NAME', 'MGM RESORTS INTERNATIONAL'), ('SRC_PARTY_SITE_NUMBER', '2601839'), ('CUSTOMER_LOCATION_ADDRESS_1', '71 E HARMON AVE'), ('CUSTOMER_LOCATION_ADDRESS_2', ''), ('CUSTOMER_LOCATION_CITY', 'LAS VEGAS'), ('CUSTOMER_LOCATION_STATE', 'NV'), ('CUSTOMER_LOCATION_ZIP', '89109-4539'), ('SRV_DEALER_REFERENCE', '00740600'), ('SRV_DEALER_NAME', 'TOSHIBA BUSINESS SOLUTIONS AZ-CO'), ('SRV_DEALER_LOCATION_ADDRESS_1', 'C/O MGM RESORTS'), ('SRV_DEALER_LOCATION_ADDRESS_2', '5014 BOND STREET'), ('SRV_DEALER_LOCATION_CITY', 'LAS VEGAS'), ('SRV_DEALER_LOCATION_STATE', 'NV'), ('SRV_DEALER_LOCATION_ZIP', '89118-1575'), ('ORIG_DEALER_REFERENCE', '00743600'), ('ORIG_DEALER_NAME', 'TABS NO COMP'), ('MODEL_NUMBER', 'ESTUDIO2802AF'), ('SERIAL_NUMBER', 'SCUBJ51625'), ('EXPIRED_FLAG', 'N'), ('ORDER_TYPE', 'LSE'), ('ORDER_NUMBER', ''), ('ORDER_DATE', ''), ('SHIPPED_DATE', ''), ('INVOICE_NO', ''), ('AUTO_CREATE_FSM', 'Processed'), ('CA_DATE', '30-APR-19'), ('SYSTEM_NAME', 'SCUBJ51625'), ('SRC_INSTANCE_ID', '63451262'), ('PURCHASE_ORDER', ''), ('ASSET_TAG_NUMBER', 'SCUBJ51625'), ('AGREEMENT', '')])\n",
      "\n",
      "Importing 2: MGM FMA 8.15.2019 Device Change Worksheet.csv as new list: data_set_2\n",
      "\u001b[4mSample pulled from MGM FMA 8.15.2019 Device Change Worksheet.csv\u001b[0m, entry from 0th list entry: \n",
      "OrderedDict([('\\ufeffDevice ID', '1884272'), ('Device Name', 'HL-3170CDW series'), ('Model', 'Brother HL-3170CDW'), ('Serial Number', 'U63478D3J153685'), ('IP Address', '10.6.64.30'), ('Last Report Date ', '8/13/2019'), ('Managed', 'Unmanaged'), ('Ship To / Company Name', ''), ('Address1', ''), ('Address 2', ''), ('Attention', ''), ('City', ''), ('State', ''), ('Zip', ''), ('Location', ''), ('Asset Number', ''), ('Cost Center', ''), ('EUConfirmation Email', ''), ('Program Type', ''), ('Black Part', ''), ('Cyan Part', ''), ('Magenta Part', ''), ('Yellow Part', '')])\n",
      "\n",
      "Importing 3: MGM Project ROS Summary.csv as new list: data_set_3\n",
      "\u001b[4mSample pulled from MGM Project ROS Summary.csv\u001b[0m, entry from 0th list entry: \n",
      "OrderedDict([('\\ufeffSheet Name', 'Borgata Implementation Rollout Schedule 2018'), ('Primary', 'MGM Borgata'), ('Location Name', ''), ('Street Address', ''), ('City', 'Atlantic City'), ('State', 'NJ'), ('Zip', '08401'), ('Floor / Reference Point', ''), ('Internal Location (Department)', 'BACK-UP UNIT'), ('Assessment Location Notes', ''), ('Toshiba Serial Number', 'SCFFH62112'), ('Toshiba Replacement Model', ''), ('ACTION', 'Install / Leave in Place'), ('STATUS for Delivery / Installation', 'COMPLETED'), ('On FMA?', ''), ('Enrolled into MPS?', ''), ('Current Device Serial Number', ''), ('IP Address (Assessment)', ''), ('Converted?', '')])\n",
      "\n",
      "Importing 4: EINVOICE-T0BUBPS-30-SEP-19.csv as new list: data_set_4\n",
      "\u001b[4mSample pulled from EINVOICE-T0BUBPS-30-SEP-19.csv\u001b[0m, entry from 0th list entry: \n",
      "OrderedDict([('\\ufeffCustomer PO#', 'ARIA'), ('Invoice#', '5065965'), ('Parent Invoice Number', '399028'), ('Invoice Date', '23-SEP-19'), ('Cont#', 'US0020919MA'), ('Status', 'ACTIVE'), ('Location', ''), ('Ship-To Name', 'ARIA'), ('Ship To Address', '3730 LAS VEGAS BOULEVARD,ARIA GENERAL MANAGER BUFFET OFFICE,,,LAS VEGAS,NV,89158-4300'), ('Ship To Address1', '3730 LAS VEGAS BOULEVARD'), ('Ship To Address2', 'ARIA GENERAL MANAGER BUFFET OFFICE'), ('Ship To Address3', ''), ('Ship To Address4', ''), ('Ship To City', 'LAS VEGAS'), ('Ship To State', 'NV'), ('Ship To Zip Code', '89158-4300'), ('Model', 'ESTUDIO2802AF'), ('Serial Number', 'SCUFG31973'), ('Meter Date Billed From', '16-NOV-18'), ('Meter Date Billed To', '30-NOV-18'), ('Base Date Billed From', ''), ('Base Date Billed To', ''), ('Meter Read Date', '30-NOV-18'), ('Meter Code', '61'), ('Counter Name', 'BW-2971876'), ('Begin Read', '0'), ('Estimated Read', ''), ('End Read', '105'), ('Usage', '105'), ('Copy Allowance', ''), ('CPC', '0.00953'), ('Excess Usage', ''), ('Excess CPC', ''), ('Excess Charge', ''), ('Base Charge', '1'), ('Total Charge', '1'), ('Tax', '0'), ('Amount Due', '1'), ('Placement Code', ''), ('Category Number', ''), ('Install Date', '16-NOV-18'), ('Customer Email', '')])\n",
      "\n",
      "Importing: TABS_DW_RPT_INSTALL_BASE_ALL_NEW_VW_20190924 BRIEF.csv as new list: data_set_large_1\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#       IMPORT SCRIPTS FOR EACH THAT EXPORTS SAMPLE OF SERIAL NUMBER\n",
    "#####################################\n",
    "\n",
    "def import_cvs_into_list(source_cvs):\n",
    "    \"\"\"\n",
    "        Imports CSV into list\n",
    "        Args:\n",
    "            source_cvs file\n",
    "        Returns:\n",
    "            data in a list variable\n",
    "    \"\"\"\n",
    "    #creates a list where each row is list entry. Each list entry is a collection of Dict's\n",
    "    data_list = []\n",
    "    with open(source_cvs, 'rb') as f:\n",
    "        reader = unicodecsv.DictReader(f)\n",
    "        for row in reader:\n",
    "            data_list.append(row)\n",
    "    #List value of row:\n",
    "    print(color.UNDERLINE + \"Sample pulled from \" + source_cvs + color.END + \", entry from 0th list entry: \")\n",
    "    print(data_list[0])\n",
    "    return data_list\n",
    "\n",
    "#####################################\n",
    "#       IMPORT SCRIPTS FOR EACH THAT EXPORTS SAMPLE OF SERIAL NUMBER [VERION 2.0]\n",
    "#####################################\n",
    "source_file_lists=list()\n",
    "source_file_lists_large=list()\n",
    "\n",
    "for num, file_name in enumerate(source_file_names, start=1):\n",
    "    print(\"\\nImporting {}: {} as new list: data_set_{}\".format(num, file_name, num))\n",
    "    locals()[\"data_set_\" + str(num)] = import_cvs_into_list(file_name)\n",
    "    source_file_lists.append(locals()[\"data_set_\" + str(num)])\n",
    "\n",
    "for num, file_name in enumerate(source_file_names_large, start=1):\n",
    "    print(\"\\nImporting: {} as new list: data_set_large_{}\".format(file_name, num))\n",
    "    locals()[\"data_set_large_\" + str(num)] = import_cvs_into_list(file_name)\n",
    "    source_file_lists_large.append(locals()[\"data_set_large_\" + str(num)])\n",
    "\n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonize data keys e.g. Serial Number columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#               Harmonize Keys      #\n",
    "#####################################\n",
    "\n",
    "def harmonize_serial_number_key(list_a):\n",
    "    \"\"\"\n",
    "    Since Serial Number is primary key, they key name sshould be the same among all tables\n",
    "    Args:\n",
    "        list_a\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    for row in list_a:\n",
    "        if ('serial_number') in row:\n",
    "            continue\n",
    "        for sn_variant in serial_number_name_variants:\n",
    "            if sn_variant in row:\n",
    "                row['serial_number'] = row[sn_variant]\n",
    "                del row[sn_variant]\n",
    "        if ('serial_number') not in row:\n",
    "            print(\"Issue: Did not find serial number key.\")\n",
    "\n",
    "for data_set in source_file_lists:\n",
    "    harmonize_serial_number_key(data_set)\n",
    "    print(data_set[0]['serial_number'])\n",
    "    \n",
    "for data_set in source_file_lists_large:\n",
    "    harmonize_serial_number_key(data_set)\n",
    "    print(data_set[0]['serial_number'])\n",
    "\n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################\n",
    "# #     Add data types other than string, IB portion and functions\n",
    "# #####################################\n",
    "\n",
    "# from datetime import datetime as dt\n",
    "\n",
    "# def validate_date(date):\n",
    "#     try:\n",
    "#         dt.strptime(date, '%m/%d/%Y')\n",
    "#         #return dt.strptime(date, '%m/%d/%Y %H:%M:%S')\n",
    "#     except ValueError:\n",
    "#         raise ValueError(\"Incorrect data format, should be %m/%d/%Y\")\n",
    "        \n",
    "# # Takes a date as a string, and returns a Python datetime object. \n",
    "# # If there is no date given, returns None\n",
    "# def parse_date(date):\n",
    "#     if date == '' or date == None:\n",
    "#         return None\n",
    "#     validate_date(date)\n",
    "#     dt.strptime(date, '%m/%d/%Y')\n",
    "    \n",
    "# # Takes a string which is either an empty string or represents an integer,\n",
    "# # and returns an int or None.\n",
    "# def parse_maybe_int(i):\n",
    "#     if i == '':\n",
    "#         return None\n",
    "#     else:\n",
    "#         return int(i)\n",
    "\n",
    "# print(\"I did the thing!\")\n",
    "    \n",
    "# print(\"----------------------------------------------------------\")\n",
    "# for data_set in source_file_lists:\n",
    "#     for date_type_column in column_data_types_dates:\n",
    "#         if date_type_column in data_set[0].keys():\n",
    "#             data_entry[date_type_column] = parse_date(str(data_entry[date_type_column]))\n",
    "\n",
    "# #     if 'INSTALL_DATE' in data_set[0].keys():\n",
    "# #         for data_entry in data_set:\n",
    "# #             data_entry['INSTALL_DATE'] = parse_date(data_entry['INSTALL_DATE'])\n",
    "# #     if 'GOLD_FLAG' in data_set[0].keys():\n",
    "# #         for data_entry in data_set:\n",
    "# #             data_entry['GOLD_FLAG'] = data_entry['GOLD_FLAG'] == 'True'\n",
    "# #     if 'Last Report Date' in data_set[0].keys():\n",
    "# #         for data_entry in data_set:\n",
    "# #             data_entry['Last Report Date'] = parse_date(data_entry['Last Report Date'])\n",
    "\n",
    "# print(\"I did the thing!\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput Duplicate Serial Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#                 Find unique and non-blank, post duplicates\n",
    "#####################################\n",
    "\n",
    "## Find the total number of rows and the number of unique students (account keys)\n",
    "## in each table.\n",
    "\n",
    "def find_unique_and_none_blank(input_list):\n",
    "    unique_values = set()\n",
    "    duplicate_values = set()\n",
    "    non_issue_duplicate_values = set()\n",
    "    blank_counter = 0\n",
    "    for row in input_list:\n",
    "        if not row['serial_number']:\n",
    "            blank_counter += 1\n",
    "        elif row['serial_number'] not in unique_values:\n",
    "            unique_values.add(row['serial_number'])\n",
    "        elif row['serial_number'] is not None:\n",
    "            #let's check if the duplicate is a non-issue\n",
    "            for non_issue_column in non_issue_duplicate_column:\n",
    "                if non_issue_column in row:\n",
    "                    #find the previously found record...\n",
    "                    for new_row in input_list: \n",
    "                        if new_row['serial_number'] == row['serial_number']: \n",
    "                            #if the values for non_issue_column in the two duplicated values do match...\n",
    "                            if new_row[non_issue_column] == row[non_issue_column]:\n",
    "                                duplicate_values.add(row['serial_number'])\n",
    "                            else:\n",
    "                                non_issue_duplicate_values.add(row['serial_number'])\n",
    "                            break\n",
    "                break\n",
    "    \n",
    "    print('Total devices checked: ' +  str(len(input_list)))\n",
    "    print('Total unique devices: ' + str(len(unique_values)))\n",
    "    print('Total blanks: ' + str(blank_counter))\n",
    "    \n",
    "    if (len(input_list)!=len(unique_values)):\n",
    "        print(\"Total duplicates: \" + str(len(input_list)-len(unique_values)))\n",
    "        print(\"Total non_issue_duplicates: \", len(non_issue_duplicate_values))\n",
    "        if(len(duplicate_values) > 1 or duplicate_values):\n",
    "            print(color.BOLD + \"Duplicate records found. Please investigate then delete applicable record(s).\" \\\n",
    "                  + color.END)\n",
    "    return duplicate_values\n",
    "    \n",
    "#####################################\n",
    "#               List orig vs duplicates count\n",
    "#####################################\n",
    "\n",
    "for list_name, data_set in zip(source_file_names,source_file_lists):\n",
    "    print (list_name)\n",
    "    print (find_unique_and_none_blank(data_set), \"\\n\")\n",
    "\n",
    "    \n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if serial in list (accounts for leading chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_serial_in_list(input_serial, check_against_list):\n",
    "    \"\"\"\n",
    "        Checks if given serial number is in a list, applies leading character if pre-defined.\n",
    "        Args:\n",
    "            input_serial\n",
    "            check_against_list\n",
    "        Returns:\n",
    "            input_serial if found (with leading if applicable)\n",
    "            None if not found\n",
    "    \"\"\"\n",
    "    if (input_serial in check_against_list):\n",
    "        return input_serial\n",
    "    for potential_leadering_char in potential_leading_chars:\n",
    "        if ((potential_leadering_char+input_serial) in check_against_list):\n",
    "    #         print('S added to beginning of Serial for ' + str(input_serial))\n",
    "            return (potential_leadering_char+input_serial)\n",
    "        elif ((input_serial[1:]) in check_against_list):\n",
    "    #         print('First Character removed to beginning of Serial for ' + str(input_serial))\n",
    "            return (input_serial[1:])\n",
    "    return None\n",
    "    \n",
    "#test above function\n",
    "print(check_serial_silly_s(\"S\"+source_file_lists[0][0]['serial_number'],\"S\" + source_file_lists[0][0]['serial_number']))\n",
    "print(check_serial_silly_s(\"S\"+source_file_lists[0][0]['serial_number'],(source_file_lists[0][0]['serial_number'])))\n",
    "print(check_serial_silly_s(source_file_lists[0][0]['serial_number'],(source_file_lists[0][0]['serial_number'])))\n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unique Asset Serial Number Lists (removing duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#     Create unique value list (remove duplicates)\n",
    "#####################################\n",
    "\n",
    "## Find any one student ib_devices where the student is missing from the daily engagement table.\n",
    "## Output that enrollment.\n",
    "\n",
    "def find_unique_and_return_list(input_list):\n",
    "    \"\"\"\n",
    "        Create unique serial number list (remove duplicates, only serial numbers returned)\n",
    "        Args:\n",
    "            input_list\n",
    "        Returns:\n",
    "            list(unique_values)\n",
    "    \"\"\"\n",
    "    unique_values = set()\n",
    "    for row in input_list:\n",
    "        if row['serial_number'] not in unique_values:\n",
    "            unique_values.add(row['serial_number'])\n",
    "    print('Total unique devices after check: ' + str(len(unique_values)))\n",
    "    if (len(input_list)!=len(unique_values)):\n",
    "        print(\"This function returned a list that removed the following number of duplicates: \" \\\n",
    "              + str(len(input_list)-len(unique_values)) + '\\n')\n",
    "    return list(unique_values)\n",
    "\n",
    "source_file_unique_serials = list()\n",
    "source_file_unique_serials_large = list()\n",
    "\n",
    "print(source_file_names)\n",
    "\n",
    "for num, data_set in enumerate(source_file_lists, start=1):\n",
    "    print(\"\\nCreating new list of unique serial numbers from file: {}. \\nNamed: data_set_unique_serials_{}\".format(source_file_names[num-1], num))\n",
    "    locals()[\"data_set_unique_serials_\" + str(num)] = find_unique_and_return_list(data_set)\n",
    "    source_file_unique_serials.append(locals()[\"data_set_unique_serials_\" + str(num)])\n",
    "    \n",
    "for num, data_set in enumerate(source_file_lists_large, start=1):\n",
    "    print(\"\\nCreating new list of unique serial numbers from file: {}. \\nNamed: data_set_unique_serials_large_{}\".format(source_file_names_large[num-1], num))\n",
    "    locals()[\"data_set_unique_serials_large_\" + str(num)] = find_unique_and_return_list(data_set)\n",
    "    source_file_unique_serials_large.append(locals()[\"data_set_unique_serials_large_\" + str(num)])\n",
    "    \n",
    "print(\"I did the thing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Records - Filter for Search Part 1: FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out(filter_keys_values_list, unique_serials_list, orig_data_set):\n",
    "    \"\"\"\n",
    "    #####################################\n",
    "        Removes entries based on filter\n",
    "        Args: \n",
    "            list_a \n",
    "            key\n",
    "            key 2 (if applicable)\n",
    "        Returns:\n",
    "            filtered_list\n",
    "    #####################################\n",
    "    \"\"\"\n",
    "    filtered_list = list()\n",
    "    removed_list = list()\n",
    "    print('\\nBefore: quantity of lists\\' entries: ' + str(len(unique_serials_list)))\n",
    "\n",
    "    # create data list using unique serial numbers:\n",
    "    full_data_unique = list()\n",
    "    for unique_entry in unique_serials_list:\n",
    "        # find orig data entry\n",
    "        for orig_entry in orig_data_set:\n",
    "            if (unique_entry == orig_entry['serial_number']):\n",
    "                full_data_unique.append(orig_entry)\n",
    "                break\n",
    "    \n",
    "    filtered_list =  full_data_unique\n",
    "    \n",
    "    for filter_item in filter_keys_values_list:\n",
    "        key = filter_item[0]\n",
    "        value = filter_item[1]\n",
    "        print(\"Removing key/value: \", key, value)\n",
    "        if key in full_data_unique[0].keys():\n",
    "            for entry in full_data_unique:\n",
    "                if entry[key] == value:\n",
    "                    #print(\"Removing: \", entry['serial_number'])\n",
    "                    removed_list.append(entry)\n",
    "                    filtered_list.remove(entry)\n",
    "                    \n",
    "    if (removed_list):\n",
    "        print(\"Number of entries removed: \" + color.RED + str(len(removed_list)) + color.END + \" out of a total \" + str(len(unique_serials_list)))\n",
    "        \n",
    "    print('After: quantity of lists\\' entries: ' + str(len(full_data_unique)))\n",
    "    return filtered_list\n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Records - Filter for Search Part 2: PERFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_file_unique_filtered_data = list()\n",
    "source_file_unique_filtered_data_large = list()\n",
    "\n",
    "for num, data_set in enumerate(source_file_lists, start=1):\n",
    "    print(\"\\nCreating new list of filtered devices: {}. \\nNamed: data_set_unique_filtered_data_{}\".format(source_file_names[num-1], num))\n",
    "    locals()[\"data_set_unique_filtered_data_\" + str(num)] = filter_out(key_and_value_to_filter_out, source_file_unique_serials[num-1], data_set)\n",
    "    source_file_unique_filtered_data.append(locals()[\"data_set_unique_filtered_data_\" + str(num)])\n",
    "    \n",
    "for num, data_set in enumerate(source_file_lists_large, start=1):\n",
    "    print(\"\\nCreating new list of unique serial numbers from file: {}. \\nNamed: data_set_unique_serials_large_{}\".format(source_file_names_large[num-1], num))\n",
    "    locals()[\"data_set_unique_serials_large_\" + str(num)] = find_unique_and_return_list(data_set)\n",
    "    #locals()[\"data_set_unique_serials_large_\" + str(num)] = filter_out(key_and_value_to_filter_out, source_file_unique_serials_large[num-1], data_set)\n",
    "    source_file_unique_filtered_data_large.append(locals()[\"data_set_unique_serials_large_\" + str(num)])\n",
    "\n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Records - Find/List Missing Serials From Each DB FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "from datetime import datetime\n",
    "\n",
    "#comparison function between two lists \n",
    "def check_and_list_missing_serials_in_lists(list_a, list_b):\n",
    "    \"\"\"\n",
    "    #####################################\n",
    "        Takes in two lists (each list is a list of dicts for a devices e.g. serial_number key to asdf1234 value)\n",
    "            and compares them to each other finding matches and \"not_found\" based on serial_number.\n",
    "        Args: \n",
    "            list_a - entries to be check, return values are based on these entries\n",
    "            list_b - entires to check against\n",
    "        Returns:\n",
    "            found_matches - a list of entries from list_a that were found in list_b based on serial_number\n",
    "            not_found - a list of entries from list_a that were NOT found in list_b based on serial_number\n",
    "    #####################################\n",
    "    \"\"\"\n",
    "    \n",
    "    #Print output \n",
    "    print(color.UNDERLINE +'\\nComparison stats:\\n' + color.END + \\\n",
    "     \" • \" + 'Quantity of primary lists\\' entries: ' + str(len(list_a)))\n",
    "\n",
    "    found_matches = list()\n",
    "    not_found = list()\n",
    "    for device in list_a:\n",
    "        device_serial = device['serial_number'] \n",
    "        if check_serial_silly_s(device_serial, list_b):\n",
    "            found_matches.append(device)\n",
    "            continue\n",
    "        else:\n",
    "            not_found.append(device)\n",
    "            \n",
    "    #Print output         \n",
    "    print(\" • \" + 'Matches found: ' + str(len(found_matches)))\n",
    "\n",
    "    if (not_found):\n",
    "        output_sample_records(not_found)\n",
    "\n",
    "    return found_matches, not_found\n",
    "\n",
    "print(\"I did the thing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Records - Find/List Missing Serials From Each OUTPUT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_sample_records(list_a):\n",
    "    \"\"\"\n",
    "    #####################################\n",
    "        Takes in one list and provides output table with applicable data for that list\n",
    "        Args: \n",
    "            list_a - entries to be output in a list.  The list contains dicts for each asset data item.\n",
    "        Returns:\n",
    "            none\n",
    "    #####################################\n",
    "    \"\"\"\n",
    "    rows = list()\n",
    "    headers = list()\n",
    "    #Print output         \n",
    "    print (\" • \" + 'Total Missing: ' + str(len(list_a)) + \" \\n\" \\\n",
    "    + color.BOLD + \"Please investigate then add applicable record(s). Some Key fields: \\n\" + color.END)\n",
    "\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    \n",
    "    for potential_key in potential_data_headers_keys:\n",
    "        if potential_key in list_a[0].keys():\n",
    "            headers.append(potential_key)\n",
    "\n",
    "    #Check if ROS headers exist and then output\n",
    "    if (headers):\n",
    "        for entry in list_a:\n",
    "            #temp new row variable to add to rows as single line\n",
    "            new_row = list()\n",
    "            for found_potential_header in headers:\n",
    "                new_row.append(entry[found_potential_header])\n",
    "            rows.append(new_row)\n",
    "                \n",
    "    #else (no header matches)\n",
    "    else:\n",
    "        headers = 'serial_number'\n",
    "        for entry in list_a:\n",
    "            rows.append(entry['serial_number'])\n",
    "\n",
    "    print(tabulate(rows, headers))\n",
    "\n",
    "print(\"I did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Records Part 1 - Find/List Missing Serials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#                 LIST MISSING SERIAL NUMBERS\n",
    "#####################################\n",
    "\n",
    "total_lists = len(source_file_lists)\n",
    "\n",
    "print(\"This section will compare each list against each other and provide missing device information.\\nThere are {} lists.\".format(total_lists))  \n",
    "\n",
    "if total_lists > 1:\n",
    "    data_set_matches_vs_missing_delta_Lists = list()\n",
    "    for num, data_set in enumerate(source_file_unique_filtered_data, start=0):\n",
    "        compare_to_index = num+1\n",
    "        locals()[\"data_set_matches_vs_missing_\" + str(num)] = list()\n",
    "        while compare_to_index < total_lists:\n",
    "            print(color.UNDERLINE +'\\nList comparison:' + color.END + ' If any devices are listed below, they are in... \\n'  + \\\n",
    "            \" • \" + \"\" + color.GREEN + source_file_names[num] + color.END + \" but are missing from... \\n\" + \\\n",
    "            \" • \" +  color.BOLD + color.RED + \"Missing from \" + source_file_names[compare_to_index] + color.END)\n",
    "            print(\"Coding troubleshooting (if needed) note: Iterator index: {}. \\ncompare_to_index: {}.\".format(num, \\\n",
    "                                                                        compare_to_index))    \n",
    "\n",
    "            data_set_matches_vs_missing_delta_Lists.append(check_and_list_missing_serials_in_lists(source_file_unique_filtered_data[num], source_file_unique_serials[compare_to_index]))\n",
    "        \n",
    "            print(\"Adding found and not-found lists to data_set_matches_vs_missing_delta_Lists to account \" + \\\n",
    "                \"for the comparison of {} to {}\".format(source_file_names[num], source_file_names[compare_to_index]))\n",
    "            compare_to_index = compare_to_index+1\n",
    "            print(\"\\nThere are now {} data_set_matches_vs_missing_delta_Lists lists.\".format(len(data_set_matches_vs_missing_delta_Lists)))\n",
    "print(\"\\nI did the thing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Records Part 2 - Reverse Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#                 LIST MISSING SERIAL NUMBERS\n",
    "#####################################\n",
    "\n",
    "#TO DO: itereate among filtered lists to check against unique serials lists\n",
    "\n",
    "total_lists = len(source_file_lists)\n",
    "\n",
    "print(\"This section will compare each list against each other and provide missing device information.\\nThere are {} lists.\".format(total_lists))  \n",
    "print(total_lists)\n",
    "if total_lists > 1:\n",
    "    for num, data_set in reversed(list(enumerate(source_file_unique_filtered_data, start=0))):\n",
    "        compare_to_index = num-1\n",
    "        locals()[\"data_set_matches_vs_missing_\" + str(num)] = list()\n",
    "        print(\"num: \", num, \". compare_to_index: \", compare_to_index)\n",
    "        while compare_to_index >= 0:\n",
    "            print(\"(within while loop) num: \", num, \". compare_to_index: \", compare_to_index)\n",
    "\n",
    "            print(color.UNDERLINE +'\\nList comparison:' + color.END + ' If any devices are listed below, they are in... \\n'  + \\\n",
    "            \" • \" + \"\" + color.GREEN + source_file_names[num] + color.END + \" but are missing from... \\n\" + \\\n",
    "            \" • \" +  color.BOLD + color.RED + \"Missing from \" + source_file_names[compare_to_index] + color.END)\n",
    "            print(\"Coding troubleshooting (if needed) note: Iterator index: {}. \\ncompare_to_index: {}.\".format(num, \\\n",
    "                                                                        compare_to_index))                \n",
    "            \n",
    "            data_set_matches_vs_missing_delta_Lists.append(check_and_list_missing_serials_in_lists(source_file_unique_filtered_data[num], source_file_unique_serials[compare_to_index]))\n",
    "        \n",
    "            print(\"Adding found and not-found lists to data_set_matches_vs_missing_delta_Lists to account \" + \\\n",
    "                \"for the comparison of {} to {}\".format(source_file_names[num], source_file_names[compare_to_index]))\n",
    "            \n",
    "            compare_to_index = compare_to_index-1\n",
    "            print(\"\\nThere are now {} data_set_matches_vs_missing_delta_Lists lists.\".format(len(data_set_matches_vs_missing_delta_Lists)))\n",
    "\n",
    "print(\"\\nI did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code checkpoint, how many lists do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"source_file_names: \", source_file_names)\n",
    "# print(\"source_file_lists Length: \", len(source_file_lists))\n",
    "# print(\"source_file_lists Length[0]: \", len(source_file_lists[0]))\n",
    "# print(\"source_file_unique_serials Length: \", len(source_file_unique_serials))\n",
    "# print(\"source_file_unique_serials[0] Length:\", len(source_file_unique_serials[0]))\n",
    "# print(\"source_file_unique_serials[0] Length:\", type(source_file_unique_serials[0]))\n",
    "# print(\"source_file_unique_filtered_data Length: \", len(source_file_unique_filtered_data))\n",
    "# print(\"source_file_unique_filtered_data[0] Length:\", len(source_file_unique_filtered_data[0]))\n",
    "# print(\"source_file_unique_filtered_data[0][0] Length: \", len(source_file_unique_filtered_data[0][0]))\n",
    "# print(\"Example of source_file_unique_filtered_data[0][0]: \", source_file_unique_filtered_data[0][0])\n",
    "\n",
    "# print(\"\\nI did the thing!\\n\")\n",
    "\n",
    "# print(\"source_file_names_large: \", source_file_names_large)\n",
    "# print(\"source_file_lists_large Length[0]: \", len(source_file_lists_large[0]))\n",
    "# print(\"source_file_lists_large Length[0][0]: \", len(source_file_lists_large[0][0]))\n",
    "# print(\"Example of source_file_lists_large[0][0]: \", source_file_lists_large[0][0])\n",
    "# print(\"source_file_unique_filtered_data_large Length: \", len(source_file_unique_filtered_data_large))\n",
    "# print(\"source_file_unique_filtered_data_large[0] Length: \", len(source_file_unique_filtered_data_large[0]))\n",
    "# print(\"Example of source_file_unique_filtered_data_large: \", source_file_unique_filtered_data_large[0][0])\n",
    "# print(\"source_file_unique_filtered_data_large[0][0] Length: \", len(source_file_unique_filtered_data_large[0][0]))\n",
    "# print(\"Example of source_file_unique_filtered_data_large[0][0]: \", source_file_unique_filtered_data_large[0][0])\n",
    "\n",
    "# print(\"\\nI did the thing!\\n\")\n",
    "\n",
    "# if (source_file_unique_filtered_data_large):\n",
    "#     print (\"source_file_unique_filtered_data_large Length: \", len(source_file_unique_filtered_data_large))\n",
    "#     print (\"source_file_unique_filtered_data_large Length[0]: \", len(source_file_unique_filtered_data_large[0]))\n",
    "#     print (\"source_file_unique_filtered_data_large Length[0][0]: \", len(source_file_unique_filtered_data_large[0][0]))\n",
    "#     print(\"Found a reference file, sample output at [0][0]: \", source_file_unique_filtered_data_large[0][0])\n",
    "# print(\"\\nI did the thing!\\n\")\n",
    "\n",
    "# if (data_set_matches_vs_missing_delta_Lists):\n",
    "#     print (\"data_set_matches_vs_missing_delta_Lists Length: \", len(data_set_matches_vs_missing_delta_Lists))\n",
    "#     print (\"data_set_matches_vs_missing_delta_Lists Length[0]: \", len(data_set_matches_vs_missing_delta_Lists[0]))\n",
    "#     print (\"data_set_matches_vs_missing_delta_Lists Length[0][0]: \", len(data_set_matches_vs_missing_delta_Lists[0][0]))\n",
    "#     print (\"data_set_matches_vs_missing_delta_Lists Length[0][1]: \", len(data_set_matches_vs_missing_delta_Lists[0][1]))\n",
    "#     print (\"data_set_matches_vs_missing_delta_Lists Length[0][0][0]: \", len(data_set_matches_vs_missing_delta_Lists[0][0][0]))\n",
    "#     print (\"data_set_matches_vs_missing_delta_Lists sample[0][0][0]: \", data_set_matches_vs_missing_delta_Lists[0][0][0])\n",
    "#     print (\"data_set_matches_vs_missing_delta_Lists sample[0][1][0]: \", data_set_matches_vs_missing_delta_Lists[0][1][0])\n",
    "\n",
    "print(\"\\nI did the thing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check missing serials against large REFERENCE file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"So, now you have a delta of the lists provided. Do you want to see if any of delta \\\n",
    "    devices are in a larger reference file? e.g. check if they are in a full DB output instead \\\n",
    "    of a select DB output\")\n",
    "print(\"\\nLet's check if we have a reference file and then if the any of the key fields are in \\\n",
    "    there that match other files...\")\n",
    "\n",
    "# TO DO: check if ref file exists and how many\n",
    "found_in_ref_file = list()\n",
    "# if there is a reference table..\n",
    "if (source_file_unique_filtered_data_large):\n",
    "    print(\"Found a reference file, sample output at [0][0]: \", source_file_unique_filtered_data_large[0][0])\n",
    "    #if there are delta list(s)...\n",
    "    if (data_set_matches_vs_missing_delta_Lists[0][1]):\n",
    "        print(\"Found a delta file, entries start at [0][1][0].\")\n",
    "        # for each of the delta lists...\n",
    "        for delta_list in data_set_matches_vs_missing_delta_Lists:\n",
    "\n",
    "            print (\"\\n\\nChecking a delta list of size: \", len(delta_list[1]), \" [Working...]\")\n",
    "            # for each of delta sublists for not_found...\n",
    "            for delta_entry_not_found in delta_list[1]:\n",
    "                #create list of keys of within the delta list to check against the keys in the reference file\n",
    "                keys_list = list()\n",
    "                for key in delta_entry_not_found.keys(): \n",
    "                    keys_list.append(key)\n",
    "                # if the entry columns match the reference columns...\n",
    "                if keys_list[0] in source_file_lists_large[0][0].keys():\n",
    "                    # if the entry serial number is found in a list of unique filtered serial numbers (based off ref file)\n",
    "                    if delta_entry_not_found['serial_number'] in source_file_unique_filtered_data_large[0]:\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        for ref_data_point in source_file_lists_large[0]: \n",
    "#                             print(delta_entry_not_found['serial_number'])\n",
    "                            if ref_data_point['serial_number'] == delta_entry_not_found['serial_number']:\n",
    "                                found_in_ref_file.append(ref_data_point)\n",
    "                                break\n",
    "#                         print(\"I found a delta data entries in the ref table: \", delta_entry_not_found[\"serial_number\"])\n",
    "#                         print(\"They have the same column: \", keys_list[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print (\"Delta assets found in Reference file (running total): \", len(found_in_ref_file))\n",
    "\n",
    "\n",
    "if(found_in_ref_file):\n",
    "    output_sample_records(found_in_ref_file)                        \n",
    "print(\"I did the thing!\")\n",
    "\n",
    "# TO DO: check ref file 1 for any unique fields in provided fields list\n",
    "\n",
    "# TO DO: check delta lists for key fields and list using delta list entries as primary list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title for next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
